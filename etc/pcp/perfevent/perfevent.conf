# File format:
# File consists of one or more groups of PMUs and the events that go with them
# ["PMU identification string"]
#   EVENT_NAME [CPU OPTION]
# where the optional CPU OPTION can be:
#    cpu to add the counter to all cpus in the system 
#    cpu_rr to add to one of the cpus in a round-robin allocation strategy
#    node to add the counter to the first cpu in each numa node
#    node_rr to add the counter to the cpus in each numa node in a round-robin allocation strategy
#    [N] where N is the numerical digit of the CPU index
#
# if the CPU option is absent it defaults to all cpus.
#
# Kernel exported PMUs can be specified in this file too under [dynamic]
# section. The PMUs exported from the kernel can be found in
# /sys/bus/event_source/devices
# and its corresponding event can be found under "events" directory inside
# the PMU directory.
# For e.g., if an event "bar"  is desired to monitored which is found here :
# /sys/bus/event_source/devices/foo/events/bar
# Then, it needs to be mentioned under [dynamic] as :
# [dynamic]
# foo.bar
#
# If not specified like above, a user will continue to see the event but
# will not be able to monitor it.
#
# For events, specified under dynamic section, the cpuconfig can also
# be specified along with the event.
# For eg:
# [dynamic]
# foo.bar [CPU NUMBER]
#
# If no cpu number is specified for events under [dynamic], then the cpumask
# specified in /sys/bus/event_source/devices/ for that event is considered.
# Suppose if cpumask is also not mentioned in /sys/ then the event is monitored
# for all cpus in the system.
#
# For hv_24x7 core level events, we can't directly read values as those events need
# additional parameter. User can specify these parameter in the following
# format:
#
# EVENT_NAME core:<core number> domain:<domain number> lpar:<lpar id>
# Here core and domain are compulsory parameter which need to be provided
# by user otherwise user will continue to see these events in list but will not be
# able to monitor it. Lpar value will be consider only if domain > 2 otherwise
# it will be ignore.
#
# To monitor hv_gpci events, user need to specify parameters based on event type.
# User can specify them in the following format:
#
# EVENT_NAME1 phys_processor_idx:<phys processor id>
# EVENT_NAME2 partition_id:<partition id>
# EVENT_NAME3 hw_chip_id:<chip number>
# EVENT_NAME4 sibling_part_id:<sibling_part_id>
# User won't be able to monitor events until it provide needed parameter data
#
# For derived events :
# [event:derived]
#   EVENT_NAME [CPU OPTION] [scale|perf_scale]
# where the CPU OPTION must match for all the events in a derived event,
# scale is a floating value and can be used to scale the event values or
# perf_scale can be used where it is known that there is a scale for a
# certain event provided in
# /sys/bus/event_source/devices/<device>/events/<event>.scale
#
# Derived events also have the capability of alternate event groups.
# [event:derived]
#    EVENT_NAME1 [CPU OPTION] [scale|perf_scale]
#    EVENT_NAME2 [CPU OPTION] [scale|perf_scale]
#    ||
#    EVENT_NAME3 [CPU OPTION] [scale|perf_scale]
#    EVENT_NAME4 [CPU OPTION] [scale|perf_scale]
#    ||
#    ...
# If specified like above, depending on the availability of the events,
# only one group will be activated ("||" is the group separator).
# First group with all the events available will be activated.
#
#
# Typically, users can specify perf events using just the event name but
# in case the event name is not identified by libpfm, they can directly
# specify the raw event code in the following format:
#   RAW:EVENT_NAME rawcode=[RAW_EVENT_CODE]
# where the RAW_EVENT_CODE is a hexadecimal event code
# It should be noted that the raw event codes are heavily architecture
# dependent and their meanings might also vary across revisions of the
# same architecture.
#

[amd64_fam10h_barcelona amd64_fam10h_shanghai amd64_fam10h_istanbul]

DATA_CACHE_REFILLS:SYSTEM
RETIRED_SSE_OPERATIONS:ALL
CPU_CLK_UNHALTED:u

HYPERTRANSPORT_LINK2:COMMAND_DWORD_SENT:DATA_DWORD_SENT:BUFFER_RELEASE_DWORD_SENT:ADDRESS_EXT_DWORD_SENT:PER_PACKET_CRC_SENT node_rr
HYPERTRANSPORT_LINK1:COMMAND_DWORD_SENT:DATA_DWORD_SENT:BUFFER_RELEASE_DWORD_SENT:ADDRESS_EXT_DWORD_SENT:PER_PACKET_CRC_SENT node_rr
HYPERTRANSPORT_LINK0:COMMAND_DWORD_SENT:DATA_DWORD_SENT:BUFFER_RELEASE_DWORD_SENT:ADDRESS_EXT_DWORD_SENT:PER_PACKET_CRC_SENT node_rr
DRAM_ACCESSES_PAGE:HIT:MISS:CONFLICT node_rr

[amd64_fam15h_interlagos]

CPU_CLK_UNHALTED:u
RETIRED_SSE_OPERATIONS:ALL
DATA_CACHE_REFILLS_FROM_NORTHBRIDGE 

RETIRED_INSTRUCTIONS
RETIRED_UOPS

[wsm_unc]

# Fixed counters
UNHALTED_REFERENCE_CYCLES
INSTRUCTIONS_RETIRED

# Generic counters
MEM_LOAD_RETIRED:L1D_HIT
FP_COMP_OPS_EXE:SSE_FP
MEM_UNCORE_RETIRED:OTHER_LLC_MISS
MEM_UNCORE_RETIRED:REMOTE_DRAM

# Uncore counters
UNC_CLK_UNHALTED      node

UNC_LLC_LINES_OUT:ANY node
UNC_LLC_LINES_IN:ANY  node
UNC_LLC_MISS:PROBE    node
UNC_LLC_MISS:WRITE    node
UNC_LLC_MISS:READ     node
UNC_LLC_HITS:PROBE    node
UNC_LLC_HITS:WRITE    node
UNC_LLC_HITS:READ     node

[wsm_dp]

# Fixed counters
UNHALTED_REFERENCE_CYCLES
INSTRUCTIONS_RETIRED

# Generic counters
MEM_LOAD_RETIRED:L1D_HIT
FP_COMP_OPS_EXE:SSE_FP
MEM_UNCORE_RETIRED:OTHER_LLC_MISS
MEM_UNCORE_RETIRED:REMOTE_DRAM

[nhm_unc]

# Fixed counters
UNHALTED_REFERENCE_CYCLES
INSTRUCTIONS_RETIRED

# Generic counters
MEM_LOAD_RETIRED:L1D_HIT
FP_COMP_OPS_EXE:SSE_FP
MEM_UNCORE_RETIRED:LOCAL_DRAM
MEM_UNCORE_RETIRED:REMOTE_DRAM

# Uncore counters
UNC_CLK_UNHALTED      node

UNC_LLC_LINES_OUT:ANY node
UNC_LLC_LINES_IN:ANY  node
UNC_LLC_MISS:PROBE    node
UNC_LLC_MISS:WRITE    node
UNC_LLC_MISS:READ     node
UNC_LLC_HITS:PROBE    node
UNC_LLC_HITS:WRITE    node
UNC_LLC_HITS:READ     node

[nhm_ex nhm]

# Fixed counters
UNHALTED_REFERENCE_CYCLES
INSTRUCTIONS_RETIRED

# Generic counters
MEM_LOAD_RETIRED:L1D_HIT
FP_COMP_OPS_EXE:SSE_FP
MEM_UNCORE_RETIRED:LOCAL_DRAM
MEM_UNCORE_RETIRED:REMOTE_DRAM

[hsw]
# Fixed Counters
UNHALTED_REFERENCE_CYCLES
INSTRUCTIONS_RETIRED

# Programmable Counters
MEM_UOPS_RETIRED:ALL_LOADS
MEM_LOAD_UOPS_RETIRED:L1_HIT
MEM_LOAD_UOPS_RETIRED:L2_HIT
MEM_LOAD_UOPS_RETIRED:L3_HIT
L1D:REPLACEMENT
DTLB_LOAD_MISSES:MISS_CAUSES_A_WALK
RESOURCE_STALLS:ANY
L2_LINES_IN:ANY

[snb_ep]
# Fixed Counters
UNHALTED_REFERENCE_CYCLES
INSTRUCTIONS_RETIRED

# UOPS issued and retired give approx idea of how much useful work the processor is doing. It is possible
# for the processor to be 100% busy according to top but most of the time is spent in stalled cycles
UOPS_ISSUED:ANY
UOPS_ISSUED:STALL_CYCLES
UOPS_RETIRED:ANY
UOPS_RETIRED:STALL_CYCLES

# Programmable counters
FP_COMP_OPS_EXE:X87
MEM_LOAD_UOPS_RETIRED:L1_HIT
FP_COMP_OPS_EXE:SSE_SCALAR_DOUBLE
FP_COMP_OPS_EXE:SSE_FP_PACKED_DOUBLE
SIMD_FP_256:PACKED_DOUBLE

# Uncore counters
snbep_unc_cbo0::UNC_C_CLOCKTICKS node
snbep_unc_cbo0::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo0::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo0::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_cbo1::UNC_C_CLOCKTICKS node
snbep_unc_cbo1::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo1::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo1::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_cbo2::UNC_C_CLOCKTICKS node
snbep_unc_cbo2::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo2::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo2::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_cbo3::UNC_C_CLOCKTICKS node
snbep_unc_cbo3::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo3::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo3::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_cbo4::UNC_C_CLOCKTICKS node
snbep_unc_cbo4::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo4::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo4::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_cbo5::UNC_C_CLOCKTICKS node
snbep_unc_cbo5::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo5::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo5::UNC_C_RXR_OCCUPANCY:IRQ node

# Note that C-Box 6 and 7 are only present on the 8-core chip variant

snbep_unc_cbo6::UNC_C_CLOCKTICKS node
snbep_unc_cbo6::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo6::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo6::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_cbo7::UNC_C_CLOCKTICKS node
snbep_unc_cbo7::UNC_C_RXR_INSERTS:IRQ node
snbep_unc_cbo7::UNC_C_COUNTER0_OCCUPANCY:e:t=1 node
snbep_unc_cbo7::UNC_C_RXR_OCCUPANCY:IRQ node

snbep_unc_ha::UNC_H_CLOCKTICKS node
snbep_unc_ha::UNC_H_TXR_BL_CYCLES_FULL:ALL node
snbep_unc_ha::UNC_H_REQUESTS:READS node
snbep_unc_ha::UNC_H_REQUESTS:WRITES node

# Up to 4 integrated memory controllers. The number of active memory controllers
# will depend on the physical memory present in the system.

snbep_unc_imc0::UNC_M_ACT_COUNT node
snbep_unc_imc0::UNC_M_PRE_COUNT:PAGE_MISS node
snbep_unc_imc0::UNC_M_CAS_COUNT:RD node # == read bandwidth / 64
snbep_unc_imc0::UNC_M_CAS_COUNT:WR node # == write bandwidth / 64

snbep_unc_imc1::UNC_M_ACT_COUNT node
snbep_unc_imc1::UNC_M_PRE_COUNT:PAGE_MISS node
snbep_unc_imc1::UNC_M_CAS_COUNT:RD node # == read bandwidth / 64
snbep_unc_imc1::UNC_M_CAS_COUNT:WR node # == write bandwidth / 64

snbep_unc_imc2::UNC_M_ACT_COUNT node
snbep_unc_imc2::UNC_M_PRE_COUNT:PAGE_MISS node
snbep_unc_imc2::UNC_M_CAS_COUNT:RD node # == read bandwidth / 64
snbep_unc_imc2::UNC_M_CAS_COUNT:WR node # == write bandwidth / 64

snbep_unc_imc3::UNC_M_ACT_COUNT node
snbep_unc_imc3::UNC_M_PRE_COUNT:PAGE_MISS node
snbep_unc_imc3::UNC_M_CAS_COUNT:RD node # == read bandwidth / 64
snbep_unc_imc3::UNC_M_CAS_COUNT:WR node # == write bandwidth / 64

snbep_unc_pcu::UNC_P_CLOCKTICKS node
snbep_unc_pcu::UNC_P_FREQ_MAX_CURRENT_CYCLES node # cycles the max frequency is limited by current
snbep_unc_pcu::UNC_P_FREQ_MAX_LIMIT_THERMAL_CYCLES node # cycles the max frequency is limited by temperature
snbep_unc_pcu::UNC_P_FREQ_MAX_POWER_CYCLES node # cycles the max frequency is limited by power

snbep_unc_r2pcie::UNC_R2_RING_BL_USED:CCW_EVEN node
snbep_unc_r2pcie::UNC_R2_RING_BL_USED:CCW_ODD node
snbep_unc_r2pcie::UNC_R2_RING_BL_USED:CW_EVEN node
snbep_unc_r2pcie::UNC_R2_RING_BL_USED:CW_ODD node

snbep_unc_r3qpi0::UNC_R3_CLOCKTICKS node
snbep_unc_r3qpi0::UNC_R3_TXR_INSERTS node
snbep_unc_r3qpi0::UNC_R3_TXR_CYCLES_FULL node

snbep_unc_r3qpi1::UNC_R3_CLOCKTICKS node
snbep_unc_r3qpi1::UNC_R3_TXR_INSERTS node
snbep_unc_r3qpi1::UNC_R3_TXR_CYCLES_FULL node

# RAPL power counters
RAPL:PACKAGE_ENERGY node
RAPL:PP0_ENERGY node
RAPL:DRAM_ENERGY node
RAPL:THERMAL_SPEC node
RAPL:MINIMUM_POWER node
RAPL:MAXIMUM_POWER node

[core]

FP_COMP_OPS_EXE:u
UNHALTED_CORE_CYCLES
INSTRUCTIONS_RETIRED
UNHALTED_REFERENCE_CYCLES
BACLEARS

[ix86arch]

UNHALTED_CORE_CYCLES
INSTRUCTION_RETIRED
UNHALTED_REFERENCE_CYCLES
LLC_MISSES

[dynamic]
cpu.branch-misses
cpu.cache-references
software.page-faults
software.context-switches
hv_24x7.PM_PB_CYC	chip:1
hv_24x7.PM_MBA0_CLK_CYC	chip:0

[current_bandwidth:derived]
snbep_unc_imc0::UNC_M_CAS_COUNT:RD node perf_scale
snbep_unc_imc1::UNC_M_CAS_COUNT:RD node perf_scale
snbep_unc_imc2::UNC_M_CAS_COUNT:RD node perf_scale
snbep_unc_imc3::UNC_M_CAS_COUNT:RD node perf_scale
snbep_unc_imc0::UNC_M_CAS_COUNT:WR node perf_scale
snbep_unc_imc1::UNC_M_CAS_COUNT:WR node perf_scale
snbep_unc_imc2::UNC_M_CAS_COUNT:WR node perf_scale
snbep_unc_imc3::UNC_M_CAS_COUNT:WR node perf_scale

# Fallthough case: the perf PMU is the generic one that is likely to be always
# present.  The cycles and instructions counters likewise should be there
[perf]
cycles
instructions
